{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f56a7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.34.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (1.26.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (10.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: moviepy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.34.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (65.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai \n",
    "%pip install python-dotenv\n",
    "%pip install torch torchvision\n",
    "%pip install pandas numpy matplotlib \n",
    "%pip install imageio\n",
    "%pip install certifi\n",
    "%pip install opencv-python moviepy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bdf4803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import imageio\n",
    "\n",
    "from openai import OpenAI \n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import moviepy.editor as mp\n",
    "import time\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c57330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99707",
   "metadata": {},
   "source": [
    "# Preprocessing gif\n",
    "converting .gif to .png by processing it into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames_from_gif(gif_path, output_folder):\n",
    "#     reader = imageio.get_reader(gif_path)\n",
    "#     for i, frame in enumerate(reader):\n",
    "#         frame_path = os.path.join(output_folder, f\"frame_{i}.png\")\n",
    "#         imageio.imwrite(frame_path, frame)\n",
    "\n",
    "# def preprocess_gif_dataset(dataset_path):\n",
    "#     for subdir, dirs, files in os.walk(dataset_path):\n",
    "#         for file in files:\n",
    "#             filepath = os.path.join(subdir, file)\n",
    "#             if filepath.endswith('.gif'):\n",
    "#                 output_folder = os.path.join(subdir, os.path.splitext(file)[0])  # Create a folder for each GIF\n",
    "#                 os.makedirs(output_folder, exist_ok=True)\n",
    "#                 extract_frames_from_gif(filepath, output_folder)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# preprocess_gif_dataset('mgif/moving-gif/train')\n",
    "# preprocess_gif_dataset('mgif/moving-gif/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a165a9",
   "metadata": {},
   "source": [
    "converting gif frams to JSON for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e3d2b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30 training examples\n",
      "Generated 9 testing examples\n"
     ]
    }
   ],
   "source": [
    "# # Function to process a video and extract frames at regular intervals\n",
    "# def process_video(video_path, seconds_per_frame=2):\n",
    "#     base64Frames = []\n",
    "#     base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "#     video = cv2.VideoCapture(video_path)\n",
    "#     total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     fps = video.get(cv2.CAP_PROP_FPS)\n",
    "#     frames_to_skip = int(fps * seconds_per_frame)\n",
    "#     curr_frame = 0\n",
    "\n",
    "#     # Loop through the video and extract frames at specified sampling rate\n",
    "#     while curr_frame < total_frames - 1:\n",
    "#         video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "#         success, frame = video.read()\n",
    "#         if not success:\n",
    "#             break\n",
    "#         _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "#         base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "#         curr_frame += frames_to_skip\n",
    "#     video.release()\n",
    "\n",
    "#     return base64Frames\n",
    "\n",
    "# # Function to process PNG frames and convert them to base64\n",
    "# def process_png_frames(dir_path, seconds_per_frame=2):\n",
    "#     base64Frames = []\n",
    "#     frames = sorted([f for f in os.listdir(dir_path) if f.endswith('.png')])\n",
    "#     frames_to_skip = int(1 / seconds_per_frame)  # Assuming 1 frame per second for simplicity\n",
    "\n",
    "#     for i, frame in enumerate(frames):\n",
    "#         if i % frames_to_skip == 0:\n",
    "#             frame_path = os.path.join(dir_path, frame)\n",
    "#             image = imageio.imread(frame_path)\n",
    "#             with BytesIO() as buffer:\n",
    "#                 imageio.imwrite(buffer, image, format='png')\n",
    "#                 base64Frames.append(base64.b64encode(buffer.getvalue()).decode('utf-8'))\n",
    "#     return base64Frames\n",
    "\n",
    "# # Function to generate fine-tuning data\n",
    "# def generate_finetuning_data(dataset_path, max_folders):\n",
    "#     examples = []\n",
    "\n",
    "#     for subdir, dirs, files in os.walk(dataset_path):\n",
    "#         for dir in dirs:\n",
    "#             dir_number = dir.lstrip(\"0\")\n",
    "#             if dir_number and int(dir_number) < max_folders:\n",
    "#                 dir_path = os.path.join(subdir, dir)\n",
    "#                 base64Frames = process_png_frames(dir_path, seconds_per_frame=1)\n",
    "#                 user_content = [\n",
    "#                     \"These are the frames from the video.\",\n",
    "#                     *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/png;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n",
    "#                 ]\n",
    "#                 assistant_content = \"MGIF\"\n",
    "#                 example = {\n",
    "#                     \"messages\": [\n",
    "#                         {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF Respond with only category (one word).\"\"\"},\n",
    "#                         {\"role\": \"user\", \"content\": user_content},\n",
    "#                         {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "#                     ]\n",
    "#                 }\n",
    "#                 examples.append(example)\n",
    "\n",
    "#     return examples\n",
    "\n",
    "# # Generate fine-tuning data\n",
    "# train_data = generate_finetuning_data('mgif/moving-gif/train', max_folders=130)\n",
    "# test_data = generate_finetuning_data('mgif/moving-gif/test', max_folders=10)\n",
    "\n",
    "# print(f\"Generated {len(train_data)} training examples\")\n",
    "# print(f\"Generated {len(test_data)} testing examples\")\n",
    "\n",
    "# # Save the generated data to JSONL files\n",
    "# with open('mgif/moving-gif/train_finetuning_data.jsonl', 'w') as train_file:\n",
    "#     for example in train_data:\n",
    "#         train_file.write(json.dumps(example) + '\\n')\n",
    "\n",
    "# with open('mgif/moving-gif/test_finetuning_data.jsonl', 'w') as test_file:\n",
    "#     for example in test_data:\n",
    "#         test_file.write(json.dumps(example) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fab242",
   "metadata": {},
   "source": [
    "# Fine tuning MGIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64853da0",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Model gpt-4o is not available for fine-tuning or does not exist.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m   file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmgif/moving-gif/train_finetuning_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m   purpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tune\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m training_file_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m----> 8\u001b[0m fine_tune_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_file_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(fine_tune_response)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Poll the fine-tuning job status until it is complete\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/fine_tuning/jobs/jobs.py:133\u001b[0m, in \u001b[0;36mJobs.create\u001b[0;34m(self, model, training_file, hyperparameters, integrations, seed, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Creates a fine-tuning job which begins the process of creating a new model from\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    a given dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fine_tuning/jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintegrations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJobCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFineTuningJob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1261\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1249\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1257\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1258\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1259\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1260\u001b[0m     )\n\u001b[0;32m-> 1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1049\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Model gpt-4o is not available for fine-tuning or does not exist.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}"
     ]
    }
   ],
   "source": [
    "# response = client.files.create(\n",
    "#   file=open(\"mgif/moving-gif/train_finetuning_data.jsonl\", \"rb\"),\n",
    "#   purpose=\"fine-tune\"\n",
    "# )\n",
    "\n",
    "# training_file_id = response.id\n",
    "\n",
    "# fine_tune_response = client.fine_tuning.jobs.create(\n",
    "#   training_file=training_file_id, \n",
    "#   model=MODEL\n",
    "# )\n",
    "\n",
    "# print(fine_tune_response)\n",
    "\n",
    "# # Poll the fine-tuning job status until it is complete\n",
    "# fine_tune_job_id = fine_tune_response['id']\n",
    "# while True:\n",
    "#     status_response = client.fine_tuning.retrieve(id=fine_tune_job_id)\n",
    "#     status = status_response['status']\n",
    "#     if status == 'succeeded':\n",
    "#         fine_tuned_model_id = status_response['fine_tuned_model']\n",
    "#         print(fine_tuned_model_id)\n",
    "#         break\n",
    "#     elif status == 'failed':\n",
    "#         raise Exception(f\"Fine-tuning job failed with status: {status_response['status']}\")\n",
    "#     else:\n",
    "#         print(f\"Fine-tuning job status: {status}\")\n",
    "#         time.sleep(30)  # wait for 30 seconds before checking again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77948b8a",
   "metadata": {},
   "source": [
    "# Process input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d29f1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in taichi/taichi_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Extracted 8 frames\n",
      "Extracted audio to taichi/taichi_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "VIDEO_PATH = \"taichi/taichi_1.mp4\"\n",
    "\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame = 0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from video if audio track is present\n",
    "    audio_path = None\n",
    "    clip = VideoFileClip(video_path)\n",
    "    if clip.audio is not None:\n",
    "        audio_path = f\"{base_video_path}.mp3\"\n",
    "        clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
    "        clip.audio.close()\n",
    "\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    if audio_path:\n",
    "        print(f\"Extracted audio to {audio_path}\")\n",
    "    else:\n",
    "        print(\"No audio track found in the video\")\n",
    "\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)\n",
    "\n",
    "transcription_text = \"\"\n",
    "if audio_path:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=open(audio_path, \"rb\"),\n",
    "    )\n",
    "    transcription_text = transcription.text\n",
    "else:\n",
    "    transcription_text = \"No audio track available to transcribe.\"\n",
    "\n",
    "# ## Generate a summary with visual and audio and classify the video\n",
    "# response = client.chat.completions.create(\n",
    "#     model=MODEL,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"\"\"You are generating a video summary. Create a summary of the provided video and its transcript. Respond in Markdown.\"\"\"},\n",
    "#         {\"role\": \"user\", \"content\": [\n",
    "#             \"These are the frames from the video.\",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "#             {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "#             ],\n",
    "#         }\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "# )\n",
    "\n",
    "# # Print summary response\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "# ## Categorize the video\n",
    "# categorization_response = client.chat.completions.create(\n",
    "#     model=MODEL,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF.\"\"\"},\n",
    "#         {\"role\": \"user\", \"content\": [\n",
    "#             \"These are the frames from the video.\",\n",
    "#             *map(lambda x: {\"type\": \"image_url\", \n",
    "#                             \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "#             {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "#             ],\n",
    "#         }\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "# )\n",
    "\n",
    "# # Print categorization response\n",
    "# print(categorization_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "596d2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaiChi\n"
     ]
    }
   ],
   "source": [
    "## Categorize the video\n",
    "categorization_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (talking head), TEDTalk (standing talking), TaiChi, MGIF Respond with only category (one word).\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Extract the classification result and store it in a variable\n",
    "category = categorization_response.choices[0].message.content.strip()\n",
    "\n",
    "# Print categorization response\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a69aa",
   "metadata": {},
   "source": [
    "# Generate Enhancement Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "889cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancement instructions: ffmpeg -i input.mp4 -vf \"minterpolate='mi_mode=mci:mc_mode=aobmc:vsbmc=1:fps=60',unsharp=5:5:0.5:5:5:0.0\" -c:v libx264 -preset slow -crf 18 -c:a aac -b:a 192k -ar 44100 -movflags +faststart output.mp4\n"
     ]
    }
   ],
   "source": [
    "# Generate enhancement instructions using OpenAI\n",
    "enhancement_prompt = f\"\"\"You are a video enhancement expert. Generate an FFmpeg script to enhance an {category} video.\"\"\"\n",
    "\n",
    "enhancement_response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:personal::9hkK0faC\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a video enhancement expert.\"},\n",
    "        {\"role\": \"user\", \"content\": enhancement_prompt}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "enhancement_instructions = enhancement_response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Print the enhancement instructions\n",
    "print(f\"Enhancement instructions: {enhancement_instructions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858ecad",
   "metadata": {},
   "source": [
    "# Execute the Enhancement Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0891430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ffmpeg -i tedtalk/ted_0705.mp4 -vf \"unsharp=5:5:2,minterpolate='mi_mode=mci:mc_mode=aobmc:vsbmc=1:fps=60'\" -c:v libx264 -preset slow -crf 18 -c:a aac -b:a 192k -ar 44100 -af \"highpass=f=200,lowpass=f=3000,acompressor=threshold=0.5:ratio=4:attack=20:release=100\" -movflags +faststart output/video_enhanced.mp4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x14662c610] Unknown cover type: 0x1.\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'tedtalk/ted_0705.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    creation_time   : 2024-07-05T20:56:50.000000Z\n",
      "  Duration: 00:00:10.40, start: 0.000000, bitrate: 6921 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1368x1080, 6913 kb/s, 30 fps, 30 tbr, 600 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-07-05T20:56:50.000000Z\n",
      "        handler_name    : Core Media Video\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : H.264\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 2 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-07-05T20:56:50.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x136605f40] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0x136605f40] profile High, level 4.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x136605f40] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=5 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=8 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=2 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=3 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=50 rc=crf mbtree=1 crf=18.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output/video_enhanced.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    encoder         : Lavf61.1.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1368x1080, q=2-31, 60 fps, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-07-05T20:56:50.000000Z\n",
      "        handler_name    : Core Media Video\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 192 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-07-05T20:56:50.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 aac\n",
      "frame=  564 fps= 13 q=26.0 size=    3328KiB time=N/A bitrate=N/A speed=N/A    ts/s speed=0.213x     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced video saved to output/video_enhanced.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mp4 @ 0x136605270] Starting second pass: moving the moov atom to the beginning of the file\n",
      "[out#0/mp4 @ 0x136605030] video:3736KiB audio:3KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.482741%\n",
      "frame=  621 fps= 14 q=-1.0 Lsize=    3756KiB time=00:00:10.31 bitrate=2982.8kbits/s speed=0.238x    \n",
      "[libx264 @ 0x136605f40] frame I:3     Avg QP:14.63  size: 90310\n",
      "[libx264 @ 0x136605f40] frame P:198   Avg QP:17.51  size: 16366\n",
      "[libx264 @ 0x136605f40] frame B:420   Avg QP:15.63  size:   746\n",
      "[libx264 @ 0x136605f40] consecutive B-frames:  0.8% 27.1%  0.0% 72.1%\n",
      "[libx264 @ 0x136605f40] mb I  I16..4: 21.4% 60.2% 18.4%\n",
      "[libx264 @ 0x136605f40] mb P  I16..4:  1.1%  2.3%  0.8%  P16..4: 25.2%  8.7%  4.9%  0.0%  0.0%    skip:57.0%\n",
      "[libx264 @ 0x136605f40] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  9.9%  0.1%  0.0%  direct: 0.3%  skip:89.7%  L0:29.7% L1:66.6% BI: 3.6%\n",
      "[libx264 @ 0x136605f40] 8x8 transform intra:56.6% inter:51.0%\n",
      "[libx264 @ 0x136605f40] direct mvs  spatial:97.9% temporal:2.1%\n",
      "[libx264 @ 0x136605f40] coded y,uvDC,uvAC intra: 47.2% 67.6% 34.0% inter: 4.5% 7.2% 0.2%\n",
      "[libx264 @ 0x136605f40] i16 v,h,dc,p: 43% 19% 14% 24%\n",
      "[libx264 @ 0x136605f40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16%  9% 30%  6%  8%  7%  8%  7%  8%\n",
      "[libx264 @ 0x136605f40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15% 10% 28%  7% 11%  8%  8%  6%  8%\n",
      "[libx264 @ 0x136605f40] i8c dc,h,v,p: 45% 21% 22% 12%\n",
      "[libx264 @ 0x136605f40] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x136605f40] ref P L0: 71.5%  7.1% 10.4%  4.5%  6.6%  0.0%\n",
      "[libx264 @ 0x136605f40] ref B L0: 87.5%  8.8%  3.4%  0.4%\n",
      "[libx264 @ 0x136605f40] ref B L1: 97.2%  2.8%\n",
      "[libx264 @ 0x136605f40] kb/s:2956.31\n",
      "[aac @ 0x1366331c0] Qavg: 65536.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def enhance_video_quality(command):\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "# Example input and output video paths\n",
    "input_video_path = VIDEO_PATH\n",
    "output_video_path = \"output/video_enhanced.mp4\"\n",
    "\n",
    "# Replace placeholders with actual file paths in the generated command\n",
    "enhancement_command = enhancement_instructions.replace(\"input.mp4\", input_video_path).replace(\"output.mp4\", output_video_path)\n",
    "\n",
    "enhancement_command = \"\"\"\n",
    "ffmpeg -i tedtalk/ted_0705.mp4 -vf \"unsharp=5:5:2,minterpolate='mi_mode=mci:mc_mode=aobmc:vsbmc=1:fps=60'\" -c:v libx264 -preset slow -crf 18 -c:a aac -b:a 192k -ar 44100 -af \"highpass=f=200,lowpass=f=3000,acompressor=threshold=0.5:ratio=4:attack=20:release=100\" -movflags +faststart output/video_enhanced.mp4\n",
    "\n",
    "\"\"\"\n",
    "print(enhancement_command)\n",
    "\n",
    "# Enhance the video quality\n",
    "enhance_video_quality(enhancement_command)\n",
    "\n",
    "print(f\"Enhanced video saved to {output_video_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
