{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f56a7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.34.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (1.26.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (10.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: moviepy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.34.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (65.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai \n",
    "%pip install python-dotenv\n",
    "%pip install torch torchvision\n",
    "%pip install pandas numpy matplotlib \n",
    "%pip install imageio\n",
    "%pip install certifi\n",
    "%pip install opencv-python moviepy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf4803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import imageio\n",
    "\n",
    "from openai import OpenAI \n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import moviepy.editor as mp\n",
    "import time\n",
    "import base64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bd7b7",
   "metadata": {},
   "source": [
    "# Preprocessing gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7f109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames_from_gif(gif_path, output_folder):\n",
    "#     reader = imageio.get_reader(gif_path)\n",
    "#     for i, frame in enumerate(reader):\n",
    "#         frame_path = os.path.join(output_folder, f\"frame_{i}.png\")\n",
    "#         imageio.imwrite(frame_path, frame)\n",
    "\n",
    "# def preprocess_gif_dataset(dataset_path):\n",
    "#     for subdir, dirs, files in os.walk(dataset_path):\n",
    "#         for file in files:\n",
    "#             filepath = os.path.join(subdir, file)\n",
    "#             if filepath.endswith('.gif'):\n",
    "#                 output_folder = os.path.join(subdir, os.path.splitext(file)[0])  # Create a folder for each GIF\n",
    "#                 os.makedirs(output_folder, exist_ok=True)\n",
    "#                 extract_frames_from_gif(filepath, output_folder)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# preprocess_gif_dataset('mgif/moving-gif/train')\n",
    "# preprocess_gif_dataset('mgif/moving-gif/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb3ece",
   "metadata": {},
   "source": [
    "Step 1: Train and Save Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d6fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 256)),  # Resize the images if necessary\n",
    "#     transforms.ToTensor(),  # Convert images to tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization parameters\n",
    "# ])\n",
    "\n",
    "\n",
    "# train_mgif_dataset = datasets.ImageFolder(root='mgif/moving-gif/train', transform=transform)\n",
    "# test_mgif_dataset = datasets.ImageFolder(root='mgif/moving-gif/test', transform=transform)\n",
    "\n",
    "# train_mgif_loader = DataLoader(train_mgif_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# test_mgif_loader = DataLoader(test_mgif_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Define a simple model for demonstration\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_classes = len(train_mgif_dataset.classes)\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training and evaluation loop\n",
    "# def train_and_evaluate_model(train_loader, test_loader, dataset_name):\n",
    "    \n",
    "#     model.train()\n",
    "#     for images, labels in train_loader:\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Finished training {dataset_name}. Evaluating model...\")\n",
    "\n",
    "#     # Save the trained model\n",
    "#     torch.save(model.state_dict(), f\"{dataset_name}_model.pth\")\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Accuracy of the model on the {dataset_name} test images: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc700b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# train_and_evaluate_model(train_mgif_loader, test_mgif_loader, 'MGif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bffb57",
   "metadata": {},
   "source": [
    "Step 2: Load the Model and Perform Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a9831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(model_path, num_classes):\n",
    "#     model = models.resnet18(pretrained=False)\n",
    "#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# def classify_images(model, dataloader):\n",
    "#     results = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in dataloader:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             results.extend(predicted.cpu().numpy())\n",
    "#     return results\n",
    "\n",
    "# # Load the model\n",
    "# model_path = 'MGif_model.pth'\n",
    "# num_classes = len(train_mgif_dataset.classes)\n",
    "# trained_model = load_model(model_path, num_classes)\n",
    "\n",
    "# # Perform inference on test data\n",
    "# test_results = classify_images(trained_model, test_mgif_loader)\n",
    "# print(test_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066afe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load mapping from 'mapping.txt'\n",
    "# def load_mapping(file_path):\n",
    "#     mapping = {}\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         for line in f:\n",
    "#             parts = line.strip().split(', ')\n",
    "#             if len(parts) == 2:  # Ensure the line has both key and value\n",
    "#                 key, value = parts\n",
    "#                 mapping[key] = value\n",
    "#             else:\n",
    "#                 print(f\"Skipping invalid line: {line.strip()}\")\n",
    "#     return mapping\n",
    "\n",
    "# mapping = load_mapping('mgif/moving-gif/mapping.txt')\n",
    "\n",
    "# # Generate descriptions based on mapping\n",
    "# class_descriptions = {idx: mapping.get(f\"{idx:05d}.gif\", \"No description available\") for idx in range(num_classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbb0fb",
   "metadata": {},
   "source": [
    "Step 3: Generate Descriptive Text Using GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1caccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(description):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": description}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "# # Generate detailed text based on classification results\n",
    "# for class_index in test_results:\n",
    "#     if class_index in class_descriptions:\n",
    "#         description = class_descriptions[class_index]\n",
    "#         detailed_text = generate_text(description)\n",
    "#         print(f\"Class {class_index}: {detailed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3a4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_completion = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Say this is a test\",\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29f1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Extracted 10 frames\n",
      "Extracted audio to tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Video Summary\n",
      "\n",
      "The video features a speaker delivering a talk on a stage with a red curtain backdrop, characteristic of a TED Talk setting. The speaker is dressed in a blue suit and is holding a clicker, indicating the use of visual aids or slides during the presentation.\n",
      "\n",
      "#### Key Points from the Transcript:\n",
      "- The speaker discusses the application of human interaction and expression to robots.\n",
      "- A future scenario involving a robot is introduced, suggesting a focus on the integration of human-like qualities in robotic technology.\n",
      "\n",
      "#### Visual Elements:\n",
      "- The speaker is seen in various frames, gesturing and engaging with the audience.\n",
      "- The TED logo is prominently displayed on the stage, reinforcing the formal and educational context of the talk.\n",
      "- The audience is visible in some frames, indicating a live presentation.\n",
      "\n",
      "#### Overall Theme:\n",
      "The talk appears to explore the intersection of human behavior and robotics, emphasizing how human-like interactions can enhance the functionality and acceptance of robots in future scenarios.\n",
      "The frames from the video and the context provided indicate that this is a TED Talk. The speaker is standing on a stage with a red background and the large \"TED\" letters are visible in one of the frames.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "VIDEO_PATH = \"tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp4\"\n",
    "\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame = 0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from video if audio track is present\n",
    "    audio_path = None\n",
    "    clip = VideoFileClip(video_path)\n",
    "    if clip.audio is not None:\n",
    "        audio_path = f\"{base_video_path}.mp3\"\n",
    "        clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
    "        clip.audio.close()\n",
    "\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    if audio_path:\n",
    "        print(f\"Extracted audio to {audio_path}\")\n",
    "    else:\n",
    "        print(\"No audio track found in the video\")\n",
    "\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)\n",
    "\n",
    "transcription_text = \"\"\n",
    "if audio_path:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=open(audio_path, \"rb\"),\n",
    "    )\n",
    "    transcription_text = transcription.text\n",
    "else:\n",
    "    transcription_text = \"No audio track available to transcribe.\"\n",
    "\n",
    "## Generate a summary with visual and audio and classify the video\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are generating a video summary. Create a summary of the provided video and its transcript. Respond in Markdown.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Print summary response\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "## Categorize the video\n",
    "categorization_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Print categorization response\n",
    "print(categorization_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "596d2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDTalk\n"
     ]
    }
   ],
   "source": [
    "## Categorize the video\n",
    "categorization_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF Respond with only category (one word).\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Extract the classification result and store it in a variable\n",
    "category = categorization_response.choices[0].message.content.strip()\n",
    "\n",
    "# Print categorization response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
