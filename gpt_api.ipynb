{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f56a7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jojotitichaya/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.34.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (1.26.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio) (10.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: moviepy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (2.34.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (65.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai \n",
    "%pip install python-dotenv\n",
    "%pip install torch torchvision\n",
    "%pip install pandas numpy matplotlib \n",
    "%pip install imageio\n",
    "%pip install certifi\n",
    "%pip install opencv-python moviepy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdf4803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import imageio\n",
    "\n",
    "from openai import OpenAI \n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import moviepy.editor as mp\n",
    "import time\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99707",
   "metadata": {},
   "source": [
    "# Preprocessing gif\n",
    "converting .gif to .png by processing it into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames_from_gif(gif_path, output_folder):\n",
    "#     reader = imageio.get_reader(gif_path)\n",
    "#     for i, frame in enumerate(reader):\n",
    "#         frame_path = os.path.join(output_folder, f\"frame_{i}.png\")\n",
    "#         imageio.imwrite(frame_path, frame)\n",
    "\n",
    "# def preprocess_gif_dataset(dataset_path):\n",
    "#     for subdir, dirs, files in os.walk(dataset_path):\n",
    "#         for file in files:\n",
    "#             filepath = os.path.join(subdir, file)\n",
    "#             if filepath.endswith('.gif'):\n",
    "#                 output_folder = os.path.join(subdir, os.path.splitext(file)[0])  # Create a folder for each GIF\n",
    "#                 os.makedirs(output_folder, exist_ok=True)\n",
    "#                 extract_frames_from_gif(filepath, output_folder)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# preprocess_gif_dataset('mgif/moving-gif/train')\n",
    "# preprocess_gif_dataset('mgif/moving-gif/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a165a9",
   "metadata": {},
   "source": [
    "converting gif frams to JSON for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3d2b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20 training examples\n",
      "Generated 29 testing examples\n"
     ]
    }
   ],
   "source": [
    "# Function to process a video and extract frames at regular intervals\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame = 0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    return base64Frames\n",
    "\n",
    "# Function to process PNG frames and convert them to base64\n",
    "def process_png_frames(dir_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    frames = sorted([f for f in os.listdir(dir_path) if f.endswith('.png')])\n",
    "    frames_to_skip = int(1 / seconds_per_frame)  # Assuming 1 frame per second for simplicity\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        if i % frames_to_skip == 0:\n",
    "            frame_path = os.path.join(dir_path, frame)\n",
    "            image = imageio.imread(frame_path)\n",
    "            with BytesIO() as buffer:\n",
    "                imageio.imwrite(buffer, image, format='png')\n",
    "                base64Frames.append(base64.b64encode(buffer.getvalue()).decode('utf-8'))\n",
    "    return base64Frames\n",
    "\n",
    "# Function to generate fine-tuning data\n",
    "def generate_finetuning_data(dataset_path, max_folders):\n",
    "    examples = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        for dir in dirs:\n",
    "            dir_number = dir.lstrip(\"0\")\n",
    "            if dir_number and int(dir_number) < max_folders:\n",
    "                dir_path = os.path.join(subdir, dir)\n",
    "                base64Frames = process_png_frames(dir_path, seconds_per_frame=1)\n",
    "                user_content = [\n",
    "                    \"These are the frames from the video.\",\n",
    "                    *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/png;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n",
    "                ]\n",
    "                assistant_content = \"MGIF\"\n",
    "                example = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF Respond with only category (one word).\"\"\"},\n",
    "                        {\"role\": \"user\", \"content\": user_content},\n",
    "                        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "                    ]\n",
    "                }\n",
    "                examples.append(example)\n",
    "\n",
    "    return examples\n",
    "\n",
    "# Generate fine-tuning data\n",
    "train_data = generate_finetuning_data('mgif/moving-gif/train', max_folders=120)\n",
    "test_data = generate_finetuning_data('mgif/moving-gif/test', max_folders=30)\n",
    "\n",
    "print(f\"Generated {len(train_data)} training examples\")\n",
    "print(f\"Generated {len(test_data)} testing examples\")\n",
    "\n",
    "# Save the generated data to JSON files\n",
    "with open('mgif/moving-gif/train_finetuning_data.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "with open('mgif/moving-gif/test_finetuning_data.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3493c5",
   "metadata": {},
   "source": [
    "# JSON Format validation\n",
    "Checking JSON format before fine-tunging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d7b92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "\n",
    "# Function to check if a string is a valid base64 encoded string\n",
    "def is_base64(sb):\n",
    "    try:\n",
    "        if isinstance(sb, str):\n",
    "            sb_bytes = bytes(sb, 'utf-8')\n",
    "        elif isinstance(sb, bytes):\n",
    "            sb_bytes = sb\n",
    "        else:\n",
    "            raise ValueError(\"Input should be a string or bytes\")\n",
    "        return base64.b64encode(base64.b64decode(sb_bytes)) == sb_bytes\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Function to validate JSON data format\n",
    "def validate_json_data(json_data):\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in json_data:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\", \"type\", \"image_url\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "\n",
    "            if message.get(\"role\") == \"user\" and isinstance(content, list):\n",
    "                for item in content:\n",
    "                    if isinstance(item, dict) and \"image_url\" in item:\n",
    "                        image_url = item[\"image_url\"].get(\"url\", \"\")\n",
    "                        if not is_base64(image_url.split(\",\")[1]):\n",
    "                            format_errors[\"invalid_base64_image\"] += 1\n",
    "            elif not content and not function_call:\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    return format_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4426588",
   "metadata": {},
   "source": [
    "# Data Warnings and Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f5fb798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mgif/moving-gif/train_finetuning_data.json\n",
      "No format errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "Distribution of num_messages_per_example:\n",
      "3: 20\n",
      "\n",
      "Distribution of num_total_tokens_per_example:\n",
      "198: 1\n",
      "199: 1\n",
      "200: 1\n",
      "202: 3\n",
      "203: 2\n",
      "204: 1\n",
      "205: 1\n",
      "206: 2\n",
      "207: 1\n",
      "208: 1\n",
      "209: 1\n",
      "210: 1\n",
      "211: 1\n",
      "212: 1\n",
      "220: 1\n",
      "228: 1\n",
      "\n",
      "Distribution of num_assistant_tokens_per_example:\n",
      "4: 20\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "Processing mgif/moving-gif/test_finetuning_data.json\n",
      "No format errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "Distribution of num_messages_per_example:\n",
      "3: 29\n",
      "\n",
      "Distribution of num_total_tokens_per_example:\n",
      "196: 1\n",
      "200: 1\n",
      "201: 2\n",
      "202: 7\n",
      "203: 2\n",
      "205: 2\n",
      "206: 4\n",
      "208: 1\n",
      "209: 1\n",
      "211: 1\n",
      "218: 1\n",
      "219: 2\n",
      "244: 1\n",
      "264: 1\n",
      "314: 1\n",
      "374: 1\n",
      "\n",
      "Distribution of num_assistant_tokens_per_example:\n",
      "4: 29\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Function to count tokens (example placeholder functions, replace with actual implementation)\n",
    "def num_tokens_from_messages(messages):\n",
    "    return sum(len(message['content']) for message in messages if 'content' in message)\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    return sum(len(message['content']) for message in messages if 'role' in message and message['role'] == 'assistant')\n",
    "\n",
    "# Function to print distribution\n",
    "def print_distribution(data, description):\n",
    "    from collections import Counter\n",
    "    counter = Counter(data)\n",
    "    print(f\"\\nDistribution of {description}:\")\n",
    "    for k, v in sorted(counter.items()):\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# Load and validate JSON data\n",
    "def process_json_data(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    format_errors = validate_json_data(data)\n",
    "    \n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "\n",
    "    for ex in data:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print(f\"\\nProcessing {file_path}\")\n",
    "    if format_errors:\n",
    "        print(\"Found format errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(\"No format errors found\")\n",
    "\n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Process training data\n",
    "process_json_data('mgif/moving-gif/train_finetuning_data.json')\n",
    "\n",
    "# Process testing data\n",
    "process_json_data('mgif/moving-gif/test_finetuning_data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77948b8a",
   "metadata": {},
   "source": [
    "# Process input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29f1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Extracted 10 frames\n",
      "Extracted audio to tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Video Summary\n",
      "\n",
      "The video features a speaker delivering a talk on a stage with a red curtain backdrop, characteristic of a TED Talk setting. The speaker is dressed in a blue suit and is holding a clicker, indicating the use of visual aids or slides during the presentation.\n",
      "\n",
      "#### Key Points from the Transcript:\n",
      "- The speaker discusses the application of human interaction and expression to robots.\n",
      "- A future scenario involving a robot is introduced, suggesting a focus on the integration of human-like qualities in robotic technology.\n",
      "\n",
      "#### Visual Elements:\n",
      "- The speaker is seen in various frames, gesturing and engaging with the audience.\n",
      "- The TED logo is prominently displayed on the stage, reinforcing the formal and educational context of the talk.\n",
      "- The audience is visible in some frames, indicating a live presentation.\n",
      "\n",
      "#### Overall Theme:\n",
      "The talk appears to explore the intersection of human behavior and robotics, emphasizing how human-like interactions can enhance the functionality and acceptance of robots in future scenarios.\n",
      "The frames from the video and the context provided indicate that this is a TED Talk. The speaker is standing on a stage with a red background and the large \"TED\" letters are visible in one of the frames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VIDEO_PATH = \"tedtalk/2024-catie-cuan-001-2590b41f-d2e2-4469-b090-5000k.mp4\"\n",
    "\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame = 0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from video if audio track is present\n",
    "    audio_path = None\n",
    "    clip = VideoFileClip(video_path)\n",
    "    if clip.audio is not None:\n",
    "        audio_path = f\"{base_video_path}.mp3\"\n",
    "        clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
    "        clip.audio.close()\n",
    "\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    if audio_path:\n",
    "        print(f\"Extracted audio to {audio_path}\")\n",
    "    else:\n",
    "        print(\"No audio track found in the video\")\n",
    "\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)\n",
    "\n",
    "transcription_text = \"\"\n",
    "if audio_path:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=open(audio_path, \"rb\"),\n",
    "    )\n",
    "    transcription_text = transcription.text\n",
    "else:\n",
    "    transcription_text = \"No audio track available to transcribe.\"\n",
    "\n",
    "## Generate a summary with visual and audio and classify the video\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are generating a video summary. Create a summary of the provided video and its transcript. Respond in Markdown.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Print summary response\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "## Categorize the video\n",
    "categorization_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Print categorization response\n",
    "print(categorization_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "596d2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDTalk\n"
     ]
    }
   ],
   "source": [
    "## Categorize the video\n",
    "categorization_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a classifier. Classify the provided video into one of the following categories: Vox (people talking), TEDTalk (standing talking), TaiChi, MGIF Respond with only category (one word).\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            \"These are the frames from the video.\",\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription_text}\"}\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Extract the classification result and store it in a variable\n",
    "category = categorization_response.choices[0].message.content.strip()\n",
    "\n",
    "# Print categorization response\n",
    "print(category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
